#!/usr/bin/env python
# Copyright 2019-2024 AstroLab Software
# Author: Julien Peloton
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Simulate batches of alerts coming from ZTF or ELaSTICC.
"""
import argparse
import os
import sys
import glob
import time
import asyncio
import gzip
import fastavro

import numpy as np

from fink_alert_simulator import alertProducer
from fink_alert_simulator import avroUtils

from fink_alert_simulator.parser import getargs

def main():
    parser = argparse.ArgumentParser(description=__doc__)
    args = getargs(parser)

    # Configure producer connection to Kafka broker
    conf = {'bootstrap.servers': args.servers}
    streamproducer = alertProducer.AlertProducer(
        args.topic, schema_files=None, **conf)

    if args.external_schema != 'None':
        schema = fastavro.schema.load_schema(args.external_schema)

    # Scan for avro files
    root = args.datasimpath

    # Grab data stored on disk
    files = glob.glob(os.path.join(root, "*.avro*"))

    # Number of observations, and total number of alerts to send.
    nobs = args.nobservations
    poolsize = args.nalerts_per_obs * nobs

    if nobs == -1:
        # Take all alerts available
        nobs = int(len(files) / float(args.nalerts_per_obs)) + 1
        poolsize = args.nalerts_per_obs * nobs
        msg = """
        All {} alerts to be sent (nobservations=-1), corresponding
        to {} observations ({} alerts each).
        """.format(len(files), nobs, args.nalerts_per_obs)
        print(msg)
    elif len(files) < poolsize:
        # Send only available alerts
        nobs = int(len(files) / float(args.nalerts_per_obs)) + 1
        msg = """
        You ask for more data than you have!
        Number of alerts on disk ({}): {}
        Number of alerts required (nalerts_per_obs * nobservations): {}

        Hence, we reduced the number of observations to {}.
        """.format(root, len(files), poolsize, nobs)
        print(msg)

    print('Total alert available ({}): {}'.format(root, len(files)))
    print('Total alert to be sent: {}'.format(poolsize))

    # Break the alert list into observations
    files = np.array_split(files[:poolsize], nobs)[:nobs]

    # Starting time
    t0 = time.time()
    print("t0: {}".format(t0))

    def send_visit_internal_schema(list_of_files):
        """ Send all alerts of an observation for publication in Kafka

        Parameters
        ----------
        list_of_files: list of str
            List with filenames containing the alert (avro file). Alerts
            can be gzipped, but the extension should be
            explicit (`avro` or `avro.gz`).
        """
        print('Observation start: t0 + : {:.2f} seconds'.format(
            time.time() - t0))
        # Load alert contents
        startstop = []
        for index, fn in enumerate(list_of_files):
            if fn.endswith('avro'):
                copen = lambda x: open(x, mode='rb')
            elif fn.endswith('avro.gz'):
                copen = lambda x: gzip.open(x, mode='rb')
            else:
                msg = """
                Alert filename should end with `avro` or `avro.gz`.
                Currently trying to read: {}
                """.format(fn)
                raise NotImplementedError(msg)

            # assign a partition number
            partition = np.random.randint(low=0, high=args.npartitions)

            with copen(fn) as file_data:
                # Read the data
                data = avroUtils.readschemadata(file_data)

                # Read the Schema
                schema = data.schema

                # assuming one record per data
                record = next(data)
                if index == 0 or index == len(list_of_files) - 1:
                    if args.to_display != 'None':
                        fields = args.to_display.split(',')
                        to_display = record[fields[0]]
                        for field_ in fields[1:]:
                            to_display = to_display[field_]
                        startstop.append(to_display)
                streamproducer.send(record, alert_schema=schema, partition=partition, encode=True)

        if args.to_display != 'None':
            print('{} alerts sent ({} to {})'.format(len(
                list_of_files),
                startstop[0],
                startstop[1]))

        # Trigger the producer
        streamproducer.flush()

    def send_visit_external_schema(list_of_files):
        """ Send all alerts of an observation for publication in Kafka

        Requires external schema to be defined.

        Parameters
        ----------
        list_of_files: list of str
            List with filenames containing the alert (avro file). Alerts
            can be gzipped, but the extension should be
            explicit (`avro` or `avro.gz`).
        """
        print('Observation start: t0 + : {:.2f} seconds'.format(
            time.time() - t0))
        # Load alert contents
        startstop = []
        for index, fn in enumerate(list_of_files):
            with open(fn, 'rb') as fp:
                record = fastavro.schemaless_reader(fp, schema)

                # assign a partition number
                partition = np.random.randint(low=0, high=args.npartitions)

                if index == 0 or index == len(list_of_files) - 1:
                    if args.to_display != 'None':
                        fields = args.to_display.split(',')
                        to_display = record[fields[0]]
                        for field_ in fields[1:]:
                            to_display = to_display[field_]
                        startstop.append(to_display)
                streamproducer.send(record, alert_schema=schema, partition=partition, encode=True)

        if args.to_display != 'None':
            print('{} alerts sent ({} to {})'.format(len(
                list_of_files),
                startstop[0],
                startstop[1]))

        # Trigger the producer
        streamproducer.flush()


    if args.external_schema != 'None':
        send_visit = send_visit_external_schema
    else:
        send_visit = send_visit_internal_schema

    loop = asyncio.get_event_loop()
    asyncio.ensure_future(
        alertProducer.schedule_delays(
            loop,
            send_visit,
            files,
            interval=args.tinterval_kafka))
    loop.run_forever()
    loop.close()


if __name__ == "__main__":
    main()
